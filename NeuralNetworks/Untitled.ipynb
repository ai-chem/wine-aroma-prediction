{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62036a09-c370-4a65-a3d7-fdeb44dabd02",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'add_safe_globals' from 'torch.serialization' (C:\\Users\\Peach\\anaconda3\\envs\\practicum\\lib\\site-packages\\torch\\serialization.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcontextlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nullcontext\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# --- это нужно, если .pt сохранили в окружении с другой версией torch ---\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserialization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m add_safe_globals\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch_version\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_tv\u001b[39;00m\n\u001b[0;32m     15\u001b[0m add_safe_globals([_tv\u001b[38;5;241m.\u001b[39mTorchVersion])\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'add_safe_globals' from 'torch.serialization' (C:\\Users\\Peach\\anaconda3\\envs\\practicum\\lib\\site-packages\\torch\\serialization.py)"
     ]
    }
   ],
   "source": [
    "# predict.py\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from contextlib import nullcontext\n",
    "\n",
    "# --- это нужно, если .pt сохранили в окружении с другой версией torch ---\n",
    "from torch.serialization import add_safe_globals\n",
    "import torch.torch_version as _tv\n",
    "add_safe_globals([_tv.TorchVersion])\n",
    "\n",
    "# ====================== Defaults ======================\n",
    "DEFAULT_LABEL_NAMES = [\n",
    "    \"Fruits\", \"Berries\", \"Flowers\", \"Herbs/Spices\", \"Wood\",\n",
    "    \"Tobacco/Smoke\", \"Citrus\", \"Nuts\", \"Coffee\", \"Chocolate/Cacao\",\n",
    "]\n",
    "\n",
    "def _resolve_label_names(n_classes: int, ckpt_names):\n",
    "    if isinstance(ckpt_names, (list, tuple)) and len(ckpt_names) == n_classes:\n",
    "        return list(map(str, ckpt_names)), True\n",
    "    if len(DEFAULT_LABEL_NAMES) == n_classes:\n",
    "        return DEFAULT_LABEL_NAMES[:], True\n",
    "    print(f\"[WARN] n_classes={n_classes} but have \"\n",
    "          f\"{len(ckpt_names) if ckpt_names is not None else 'None'} ckpt names \"\n",
    "          f\"and {len(DEFAULT_LABEL_NAMES)} defaults; using generic class_i.\")\n",
    "    return [f\"class_{i}\" for i in range(n_classes)], False\n",
    "\n",
    "# ====================== Model ======================\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, depth=2, n_classes=10, p=0.2, widths=(64,128,256,512,512)):\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "        layers, in_ch = [], 1\n",
    "        for i in range(depth):\n",
    "            out_ch = widths[i]\n",
    "            layers += [\n",
    "                nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "                nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            ]\n",
    "            in_ch = out_ch\n",
    "        # адаптивный пул → не зависит от входного HxW\n",
    "        self.features = nn.Sequential(*layers, nn.AdaptiveAvgPool2d(1))\n",
    "        self.fc1 = nn.Linear(in_ch, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, n_classes)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)              # (B, C, 1, 1)\n",
    "        x = x.view(x.size(0), -1)        # (B, C)\n",
    "        x = F.leaky_relu(self.fc1(x)); x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        return self.fc3(x)               # logits\n",
    "\n",
    "# ====================== Helpers ======================\n",
    "def _to_chw1(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Expect [H,W] or [1,H,W]; return standardized [1,H,W] float32.\"\"\"\n",
    "    if x.ndim == 2:\n",
    "        x = x[None, ...]\n",
    "    elif not (x.ndim == 3 and x.shape[0] == 1):\n",
    "        raise ValueError(f\"Expected [H,W] or [1,H,W], got {x.shape}\")\n",
    "    x = x.astype(np.float32, copy=False)\n",
    "    m, s = x.mean(), x.std()\n",
    "    if s == 0: s = 1.0\n",
    "    return (x - m) / (s + 1e-6)\n",
    "\n",
    "def _find_best_ckpt(weights_dir: Path) -> Path:\n",
    "    cands = list(weights_dir.rglob(\"*.pt\"))\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(f\"No .pt files under {weights_dir}\")\n",
    "    have_any_metrics = False\n",
    "    scored = []\n",
    "    for p in cands:\n",
    "        try:\n",
    "            ckpt = torch.load(p, map_location=\"cpu\")\n",
    "            m = ckpt.get(\"metrics\", {}) or {}\n",
    "            vl = float(m.get(\"val_loss\", math.inf))\n",
    "            va = float(m.get(\"val_acc\", -math.inf))\n",
    "            if math.isfinite(vl) or math.isfinite(va):\n",
    "                have_any_metrics = True\n",
    "        except Exception:\n",
    "            vl, va = math.inf, -math.inf\n",
    "        mt = p.stat().st_mtime\n",
    "        scored.append((p, vl, va, mt))\n",
    "    if have_any_metrics:\n",
    "        scored.sort(key=lambda t: (t[1], -t[2], -t[3]))\n",
    "        return scored[0][0]\n",
    "    return max(cands, key=lambda p: p.stat().st_mtime)\n",
    "\n",
    "@torch.no_grad()\n",
    "def _predict_array(model: nn.Module, device: str, arr: np.ndarray, threshold: float):\n",
    "    x = torch.from_numpy(_to_chw1(arr))[None].to(device)  # (1,1,H,W)\n",
    "    use_amp = (device == \"cuda\")\n",
    "    ctx = torch.amp.autocast(device_type=\"cuda\", enabled=use_amp) if use_amp else nullcontext()\n",
    "    with ctx:\n",
    "        logits = model(x)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()[0]\n",
    "    preds = (probs >= threshold).astype(int)\n",
    "    return probs, preds\n",
    "\n",
    "def _infer_arch_from_state_dict(sd: dict, depth_fallback: int = 2):\n",
    "    \"\"\"Infer (n_classes, depth) from state_dict when arch info is absent.\"\"\"\n",
    "    if \"fc3.weight\" in sd and isinstance(sd[\"fc3.weight\"], torch.Tensor):\n",
    "        n_classes = int(sd[\"fc3.weight\"].shape[0])\n",
    "    else:\n",
    "        raise ValueError(\"Cannot infer n_classes: 'fc3.weight' not found; pass --n_classes\")\n",
    "    # count conv weights inside features.*.weight (4D tensors) → depth\n",
    "    conv_keys = [k for k,v in sd.items()\n",
    "                 if k.startswith(\"features.\") and k.endswith(\".weight\") and isinstance(v, torch.Tensor) and v.ndim == 4]\n",
    "    depth = max(1, len(conv_keys)) if conv_keys else depth_fallback\n",
    "    return n_classes, depth\n",
    "\n",
    "def _extract_state_dict(obj):\n",
    "    \"\"\"Accept various checkpoint formats and return state_dict + meta.\"\"\"\n",
    "    label_names, threshold, arch = None, 0.5, {}\n",
    "    if isinstance(obj, dict):\n",
    "        # common keys\n",
    "        for k in (\"model_state_dict\", \"model_state\", \"state_dict\", \"weights\"):\n",
    "            if k in obj and isinstance(obj[k], dict) and all(isinstance(v, torch.Tensor) for v in obj[k].values()):\n",
    "                sd = obj[k]\n",
    "                break\n",
    "        else:\n",
    "            # maybe it's already a raw state_dict (all tensor values)\n",
    "            if all(isinstance(v, torch.Tensor) for v in obj.values()):\n",
    "                sd = obj\n",
    "            else:\n",
    "                raise ValueError(\"Checkpoint dict doesn't contain a recognizable state_dict.\")\n",
    "        arch = obj.get(\"arch\", {}) or {}\n",
    "        label_names = obj.get(\"label_names\", None)\n",
    "        if \"sigmoid_threshold\" in obj:\n",
    "            try: threshold = float(obj[\"sigmoid_threshold\"])\n",
    "            except Exception: pass\n",
    "    else:\n",
    "        # raw state_dict saved directly\n",
    "        if isinstance(obj, dict) and all(isinstance(v, torch.Tensor) for v in obj.values()):\n",
    "            sd = obj\n",
    "        else:\n",
    "            # torch.save(model.state_dict()) returns a dict -> obj is dict\n",
    "            sd = obj\n",
    "    return sd, arch, label_names, threshold\n",
    "\n",
    "def _load_model_and_meta(ckpt_path: Path, device: str, depth_arg: int = None, n_classes_arg: int = None):\n",
    "    obj = torch.load(ckpt_path, map_location=device)\n",
    "    state_dict, arch, label_names, threshold = _extract_state_dict(obj)\n",
    "\n",
    "    # prefer args → arch → infer\n",
    "    n_classes = n_classes_arg if n_classes_arg is not None else arch.get(\"n_classes\", None)\n",
    "    depth     = depth_arg if depth_arg is not None else arch.get(\"depth\", None)\n",
    "\n",
    "    if n_classes is None or depth is None:\n",
    "        n_infer, d_infer = _infer_arch_from_state_dict(state_dict)\n",
    "        if n_classes is None: n_classes = n_infer\n",
    "        if depth is None:     depth = d_infer\n",
    "\n",
    "    model = ConvNet(depth=int(depth), n_classes=int(n_classes)).to(device).eval()\n",
    "    missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
    "    if missing or unexpected:\n",
    "        print(f\"[WARN] missing keys: {missing} | unexpected keys: {unexpected}\")\n",
    "    return model, int(depth), int(n_classes), label_names, float(threshold)\n",
    "\n",
    "# ====================== Main ======================\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser(description=\"Multilabel ConvNet inference\")\n",
    "    ap.add_argument(\"--ckpt\", type=str, default=\"cnn_weights.pt\",\n",
    "                    help=\"Path to .pt (state_dict or checkpoint). Default: cnn_weights.pt\")\n",
    "    ap.add_argument(\"--weights_dir\", type=str, default=\"checkpoints\",\n",
    "                    help=\"If --ckpt not exists, pick best under this directory\")\n",
    "    g = ap.add_mutually_exclusive_group(required=True)\n",
    "    g.add_argument(\"--input_npy\", type=str, help=\"Path to a single .npy\")\n",
    "    g.add_argument(\"--input_csv\", type=str, help=\"CSV with column 'path' (.npy files)\")\n",
    "    ap.add_argument(\"--output_csv\", type=str, default=\"predictions.csv\", help=\"Output CSV path\")\n",
    "    ap.add_argument(\"--threshold\", type=float, default=None, help=\"Override sigmoid threshold (default from ckpt or 0.5)\")\n",
    "    ap.add_argument(\"--device\", type=str, choices=[\"auto\",\"cpu\",\"cuda\"], default=\"auto\")\n",
    "    ap.add_argument(\"--depth\", type=int, default=None, help=\"If pure state_dict and cannot be inferred\")\n",
    "    ap.add_argument(\"--n_classes\", type=int, default=None, help=\"If pure state_dict and cannot be inferred\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    device = \"cuda\" if (args.device == \"auto\" and torch.cuda.is_available()) else (args.device if args.device != \"auto\" else \"cpu\")\n",
    "\n",
    "    ckpt_path = Path(args.ckpt)\n",
    "    if not ckpt_path.exists():\n",
    "        best_path = Path(args.weights_dir) / \"best.pt\"\n",
    "        ckpt_path = best_path if best_path.exists() else _find_best_ckpt(Path(args.weights_dir))\n",
    "\n",
    "    model, depth, n_classes, label_names, thr_ckpt = _load_model_and_meta(\n",
    "        ckpt_path, device, depth_arg=args.depth, n_classes_arg=args.n_classes\n",
    "    )\n",
    "    threshold = float(thr_ckpt if args.threshold is None else args.threshold)\n",
    "    cols, _ = _resolve_label_names(n_classes, label_names)\n",
    "\n",
    "    # ---------- single file ----------\n",
    "    if args.input_npy:\n",
    "        arr = np.load(args.input_npy)\n",
    "        probs, preds = _predict_array(model, device, arr, threshold)\n",
    "        df = pd.DataFrame([probs], columns=cols)\n",
    "        for i, c in enumerate(cols):\n",
    "            df[f\"pred_{c}\"] = int(preds[i])\n",
    "        df.insert(0, \"path\", args.input_npy)\n",
    "        Path(args.output_csv).parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(args.output_csv, index=False)\n",
    "        print(f\"[OK] ckpt={ckpt_path}  saved={args.output_csv}\")\n",
    "        return\n",
    "\n",
    "    # ---------- batch from CSV ----------\n",
    "    df_in = pd.read_csv(args.input_csv)\n",
    "    if \"path\" not in df_in.columns:\n",
    "        raise ValueError(\"input_csv must contain column 'path'\")\n",
    "    rows = []\n",
    "    for p in df_in[\"path\"].tolist():\n",
    "        arr = np.load(p)\n",
    "        probs, preds = _predict_array(model, device, arr, threshold)\n",
    "        rows.append((p, probs, preds))\n",
    "\n",
    "    proba_mat = np.vstack([r[1] for r in rows])\n",
    "    pred_mat  = np.vstack([r[2] for r in rows]).astype(int)\n",
    "\n",
    "    df_out = pd.DataFrame(proba_mat, columns=cols)\n",
    "    for i, c in enumerate(cols):\n",
    "        df_out[f\"pred_{c}\"] = pred_mat[:, i]\n",
    "    df_out.insert(0, \"path\", [r[0] for r in rows])\n",
    "\n",
    "    Path(args.output_csv).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_out.to_csv(args.output_csv, index=False)\n",
    "    print(f\"[OK] ckpt={ckpt_path}  saved={args.output_csv}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22e3106-7864-473f-b78d-a9b4e92641a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
